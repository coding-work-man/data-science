
#第八章 大数据？了不起！

在2012年，许多科技媒体的头条关注了大数据。是什么让数据变大，为什么“大”重要？在这一章，我们讨论这些问题后面的争议。掌握了前一章的知识后，对于数据的规模如何影响我们的数据工作，我们可以有更多的思考。

市场观察（一个华尔街期刊）最近发表了一篇文章标题为“谷歌IT和商务专业人士指出大数据等于大回报”，副标题“根据新的全球调查，70%的组织正在考虑、计划或者运行大数据项目”。相似的技术新闻在这几年屡见不鲜。如此大量这样的文章让人不由得认为“大数据”是一种给这个世界掀起信息和技术浪潮的革命。但这是真的的？“大数据”真的改变了一切吗？

商业分析师Doug Laney指出三个特征让大数据跟以往的技术革命不同：规模，效率，多样性。规模指的是庞大的数据量。效率则是数据的快速生成和更新。最后，多样性反映会有多种不同类型的数据。这三个特性经常指代大数据的“3V”模型。但是，即使在计算机时代的破晓，我们就已经拥有多样的数据，有的数据产生非常快，而且经过时间的推移，这些数据能装满大量的存储器（试想，从18世纪开始，美国国会图书馆每年产生的多种的大量的数据）。所以，光凭某人说他们面临大规模的、高效率的、多样性的数据问题，很难说大数据是一个崭新的事物。

有这样的说法，和几年前相比，许多进行中的改变让今天的数据问题有了质的不同。让我们列下几个准确的例子： 

1. 传感器（比如条形码阅读器）价格的下降和最近几年是它价格下降并且更容易收集更多数据的技术
2. 相似的，存储器价格的下降使不管质量和使用地保存大量无关紧要的数据变得现实了。
3. 许多人对隐私的态度不再像以前那样严格，他们似乎已经适应了使用Facebook和其他会透露许多个人信息的平台。
4. 研究者已经在机器学习算法中取得了重大进步，它是构建许多数据挖掘的基础。
5. 当一个数据集达到一定规模（几千行），传统的统计显著性检验已经没有意义了，因为即使最小最微不足道的结果（或者效应量，按照统计学家的说法）也是统计上显著的


出了以上这几点，我们能然有许多需要做出的改变：  

A.垃圾进，垃圾出：数据的用处非常依赖它是如何收集的。数据收集后，它的质量非常依赖我们如何对它做预处理：数据清理和数据筛选。   
B.大等于怪异：如果你寻找异常现象——违反规则的罕见事件——那么数据量越大越好。低频率事件经常不会出现直到数据收集持续很长一段时间，或者包含一个足够大的样例组来专门记录一个奇特案例。   
C.连接增加可能：单个数据集不管提供什么变量都有内在的限制。但是如果这些数据能连接到别的数据，新的分析途径也许会突然出现。虽然不能保证，但是你连接越多的数据，你就越有可能有新的发现。   

以上两个列表的观点都是老生常谈毫无争议的。但是他们进一步解释了大数据可能会有多重要的问题。历史上已经有许多成功应用传统统计检验适当规（比如1000行或更少）模数据集的统计规律。每个人都喜欢的基础统计，学生t检验，是检验两组数据中心趋势异同的工具。如果数据包含某种规律，一组数据显著不同于另一组数据，一个t检验会告诉我们同样的结果。

大数据没办法用这样的检验帮助我们。我们甚至不需要一千条数据来做传统统计比较，而且上百万上千万的数据并不会让我们的工作更简单（它只会占用更多的电脑内存和磁盘）。想想我们前一张读到的：我们可以开始用基本的统计推断处理有51条记录的数据。实际上，大部分经常使用的统计技术，比如学生t检验，是专门设计处理小样本的。

另一方面，如果我们是在草堆里找一根针，尽量在最大的草堆里找就讲得通了。因为大的草堆更有可能包含至少一根针。随着机器学习近几年的进步，我们开始意识到好的工具、大量数据和有趣的问题确实能为我们提供新的更深刻的见解。  

让我们结合这个这样的乐观主义和三个非常重要的注意事项。第一条是当这个数据越复杂，就越难确定数据是否“干净”和适合我们的要求。一个脏数据集在某方面比没有数据还糟，因为我们也许会花很多时间精力在上面而得不到任何结果。更严重的是我们花时间精力最后只能得到错误的结果！很多分析师认为清理数据——让数据适合分析，清楚异常数据，组织适合的数据结构——事实上是数据分析过程中最耗时间精力的。

第二条注意事项是稀少和异常事件或模式一直伴随这他们不可预测的本质。甚至面对我们能想象的最好的数据和大量变量，我们几乎一直都会有准确性的问题。数据挖掘工具也许能给我们呈现数据的模式，我们甚至可以在新数据中重现这种模式，但是我们永远不敢保证我们已经对这种模式理解到能分离，控制或明白它的原因。预测飓风的路径就是一个很好的例子：尽管气象仪器，预测和数字捣弄已经有了几十年的发展，气象学家在预测飓风是否会登陆或风速会多大都面对非常大的困难。工作中的复杂性和不可预测性使得任务异常艰难。

第三条注意事项是有关连接数据集的。上文的C条目提出连接能提供格外的价值。但是每一条到新数据集的连接也增加了数据的复杂性，同时也增加了出现脏数据和假结果的可能性。而且，虽然很多公司都越来越不在乎这一点，我们连接越多个人的数据（如消费者，病人，投票者等），我们就越可能造成隐私泄露的灾难。即使你不在乎隐私问题，但是你不得不承认安全和隐私丑闻会给公司带来巨大的金钱和名誉损失。现在有价值和可接受的数据革新领域也许就是犯罪和欺诈数据。越多数据集的连接就越容易暴露那些人的恶行。

总的来说，我们可以认为丰富的大量的数据结合适合的分析工具在有保障的环境下可以为商业部门、教育部门、政府部门和其他领域带来益处。但是，数据科学家关注的不应该是取得尽可能多的可用数据，而是根据目的使用对的数据和对的规模。拥有大量不适当的数据并不会有任何好处。就好像，简单快速取得的数据并不能保证与要研究的问题相关。虽然说多样性是生活的调味品，但是过于复杂经常会损害可靠性和可信性：我们连接的数据集越复杂，我们越可能在使用和保存它们的时候出现意外问题。

##数据科学的工具

经过前面几章，我们对于上千数据分析师使用的分析工具——数据分析和数据可视化的开源软件R——已经有了一个快速的了解。尽管R功能十分强大，但是数据分析师根据研究的不同领域仍然使用了上百种其他分析工具。

出了R之外，一个非常流行和强大的工具是SAS（读“sass”），它是一个一个有专利的统计系统。SAS包含了强大的编程语言，覆盖了许多数据类型，功能和语言特点。跟R相比，学习SAS可以说难（或简单，根据你的预期），但是许多大企业使用SAS，因为不像R，SAS提供了大量的技术和产品服务。当然，这样的服务价格不便宜，所以大部分SAS使用者都是有能力购买许可证书和售后支持的大企业。

另一个在统计领域的工具是SPSS，一个许多研究者使用的工具包（全名是统计产品与服务解决方案）。对许多分析师来说，SPSS比SAS更容易上手，但是没有那么灵活和强大。

R、SPSS、SAS作为统计工具包，但数据分析师在某些方面仍然使用了其他一般编程语言做数据分析。一个令人激动的发展中的语言有一个奇怪的名字“Processing”。Processing是一个专门用来做数据可视化的编程语言。像R一样，Processing是一个开源项目，它免费提供在[http://processing.org/](http://processing.org/)。而且跟R一样，processing是一个跨平台语言，它可以在Mac、Windows、Linux上完美运行。有许多书提供processing的学习（不幸的是还没有开源书），许多网站为初学者提供了大量实例。在R之外，processing也许是数据科学家工具箱中最重要的工具之一。

##本章练习

查看各种与“Data.gov”相关的网站，尽可能找到最大或者最复杂的数据集。试思考（或写下来）一种或多种这些数据可能在分析中被误用的情况。下载一个你感兴趣的数据集，将它载入R中看你能做点什么。

下面是一个格外挑战，打开下面这个网页：  
[heep://teamwpc.co.uk/products/wps](heep://teamwpc.co.uk/products/wps)  
下载试用版的“World Programming System”（WPS）。WPS能读取SAS代码，你能很方便的找到你要的代码用来读取Data.gov数据集。


##参考资料

[http://aqua.nasa.gov/doc/pubs/Wx_Forecasting.pdf](http://aqua.nasa.gov/doc/pubs/Wx_Forecasting.pdf)  
[http://en.wikipedia.org/wiki/Big_data](http://en.wikipedia.org/wiki/Big_data)   
[http://en.wikipedia.org/wiki/Data.gov](http://en.wikipedia.org/wiki/Data.gov)   
[http://www.marketwatch.com/story/big-data-equals-big-busines
s-opportunity-say-global-it-and-business-professionals-2012-05-14](http://www.marketwatch.com/story/big-data-equals-big-busines
s-opportunity-say-global-it-and-business-professionals-2012-05-14)
